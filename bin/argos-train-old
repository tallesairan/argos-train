#!/bin/sh

echo "Preparing Data"

mkdir run
./prepare_data.py raw_data/source raw_data/target

echo "Training tokenizer"

cat run/split_data/*train.txt >> run/split_data/all.txt

 
spm_train --input=run/split_data/all.txt --model_prefix=run/sentencepiece \
           --vocab_size=$vocab_size --character_coverage=$character_coverage \
	   --input_sentence_size=50481078 --shuffle_input_sentence=true \
	   --user_defined_symbols=$special_tokens

rm run/split_data/all.txt
 

onmt_build_vocab -config config.yml -n_sample -1 --num_threads 125

echo "Done with tokenization"


# Resume train
#onmt_train  -config config.yml --num_threads 19 -train_steps 8001 -update_vocab none -train_from run/openmt.model_step_8000.pt


onmt_train -config config.yml --num_threads 19



#./../OpenNMT-py/tools/average_models.py -m  run/openmt.model_step_7000.pt   run/openmt.model_step_8000.pt -o run/averaged.pt


#ct2-opennmt-py-converter --model_path run/averaged.pt --output_dir run/model --quantization int8